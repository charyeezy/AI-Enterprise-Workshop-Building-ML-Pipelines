# AI-Enterprise-Workshop-Building-ML-Pipelines
In this workshop we are going to use Nvidia's Triton Inference Server (formerly known as TensorRT Inference Server) 
which simplifies the deployment of AI models at scale in production. For the purpose of this examination, we focus on hosting pretrained (on ImageNet) image classification models like InceptionNet, MobileNet etc. 


![1_126iG2mnfl4i6iH9FKu3sg](https://user-images.githubusercontent.com/40523048/120965914-c4a98380-c765-11eb-86f0-eb2ce2574e97.png)
image credit: https://developer.nvidia.com/nvidia-triton-inference-server?ncid=partn-88872#cid=dl13_partn_en-us 
