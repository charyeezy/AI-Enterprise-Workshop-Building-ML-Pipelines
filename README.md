# AI-Enterprise-Workshop-Building-ML-Pipelines
In this workshop we are going to use Nvidia's Triton Inference Server (formerly known as TensorRT Inference Server) 
which simplifies the deployment of AI models at scale in production.
