{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c8c20b",
   "metadata": {},
   "source": [
    "# How to trace/convert Transformer model into Triton acceptable models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00409d",
   "metadata": {},
   "source": [
    "Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93425958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Load and Convert Hugging Face Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "model = AutoModel.from_pretrained('deepset/sentence_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be78c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy inputs for tracing\n",
    "sentence = 'Who are you voting for in 2020?'\n",
    "labels = ['business', 'art & culture', 'politics']\n",
    "\n",
    "# run inputs through model and mean-pool over the sequence\n",
    "# dimension to get sequence-level representations\n",
    "inputs = tokenizer.batch_encode_plus([sentence] + labels,\n",
    "                                     return_tensors='pt', max_length=256,\n",
    "                                     truncation=True, padding='max_length')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd34ceb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input shapes\n",
    "input_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658fd3c",
   "metadata": {},
   "source": [
    "# Tracing PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd1763",
   "metadata": {},
   "source": [
    "Conversion of the model is done using its JIT traced version. According to PyTorch’s documentation: ‘Torchscript’ is a way to create serializable and optimizable models from PyTorch code”. It allows the developer to export their model to be re-used in other programs, such as efficiency-oriented C++ programs. Exporting a model requires: Dummy inputs and Standard length to execute the model’s forward pass. During the model’s forward pass with dummy inputs, PyTorch keeps the track of different operations on each tensor and records these operations to create the “trace” of the model. Since the created trace is relative to the dummy input dimensions, therefore the model inputs in the future will be constrained by the dimension of the dummy input, and will not work for other sequences length or batch size. It is therefore recommended to trace the model with the largest dummy input dimension that you can think can be fed to the model in the future. Apart from this, we can always use padding or truncation on input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e037f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained('deepset/sentence_bert')\n",
    "    def forward(self, data, attention_mask=None):\n",
    "        return self.model(data, attention_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c5c552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/sentence_bert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/sachin/anaconda3/envs/transformers/lib/python3.8/site-packages/transformers/modeling_utils.py:1967: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert all(\n"
     ]
    }
   ],
   "source": [
    "# after trace it will save the model in cwd\n",
    "pt_model = PyTorch_to_TorchScript().eval()\n",
    "traced_script_module = torch.jit.trace(pt_model, (input_ids, attention_mask), strict=False)\n",
    "traced_script_module.save(\"./model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675dc98f",
   "metadata": {},
   "source": [
    "# Next, save the model in the model repository folder with the following directory structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190c73c",
   "metadata": {},
   "source": [
    "model_repository_path/\n",
    "|- <pytorch_model_name>/\n",
    "|  |- config.pbtxt\n",
    "|  |- 1/\n",
    "|     |- model.pt\n",
    "|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ffa79",
   "metadata": {},
   "source": [
    "# Writing the Model Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb62e7",
   "metadata": {},
   "source": [
    "This configuration file, config.pbtxt contains the detail of permissible input/outputs types and shapes, favorable batch sizes, versioning, platform since the server doesn't know details about these configurations, therefore, we write them into a separate configuration file. </br>\n",
    "\n",
    "Configuration file for Hugging Face DeepSentence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f39dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: \"deepset\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [4, 256]\n",
    "  } ,\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [4, 256]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [4, 256, 768]\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers] *",
   "language": "python",
   "name": "conda-env-transformers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
